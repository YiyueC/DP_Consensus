{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DP_Consensus.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPCFaSxo7ZPYsJ4fnqTISPw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vzcZT6AYBBwo","colab_type":"code","colab":{}},"source":["import tarfile\n","from google.colab import drive\n","# drive.mount('/content/drive/')\n","!fusermount -u drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCEqFdfOBVRM","colab_type":"code","colab":{}},"source":["import numpy as np\n","from numpy import linalg as LA\n","import matplotlib.pyplot as plt\n","import random\n","import math as ma\n","import time\n","from statistics import mean \n","from itertools import combinations\n","import csv\n","import codecs\n","import zipfile as zipfile\n","from sklearn.decomposition import PCA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PDliCQmBWfh","colab_type":"code","colab":{}},"source":["# Generate data\n","\n","paraN=64\n","agentN=10\n","dataN=200\n","sampleN=agentN*dataN\n","mu=np.zeros((1, sampleN))\n","Sigma=np.eye(sampleN)\n","np.random.seed(0)\n","M=np.random.multivariate_normal(mu[0],Sigma,paraN).T\n","e=np.random.normal(0, 1, sampleN)\n","\n","# Normalize data\n","M =[ma.sqrt(int(paraN))* (np.sqrt(np.abs(LA.eig(np.outer(M[i], M[i]))[0]/ max(LA.eig(np.outer(M[i], M[i]))[0])))).dot(LA.eig(np.outer(M[i], M[i]))[1].T) for i in range(sampleN)]\n","M=np.real(M)\n","M=np.array(M)\n","#e=(2/(np.abs(e)).max(axis=0))*e\n","#e=np.zeros((sampleN, 1))\n","e=0.1*e/LA.norm(e)\n","x_star_nonnorm=np.random.random(size=paraN)-0.5\n","x_star_norm=300*x_star_nonnorm/LA.norm(x_star_nonnorm)\n","x_star=np.array(x_star_norm)\n","y=[M.dot(x_star.T)[i]+e[i] for i in range(sampleN)]\n","y=np.array(y)\n","y=y.reshape((sampleN, 1))\n","y_local=[y[i*dataN:(i+1)*dataN, :] for i in range(agentN)]\n","y_local=np.array(y_local)\n","M_local=[M[i*dataN:(i+1)*dataN, :] for i in range(agentN)] # generate groups of local data\n","M_local=np.array(M_local)\n","x_star=np.array([x_star for j in range(agentN)])\n","\n","# Construct the graph\n","def generate_w(Idx, agentN):\n","    G=np.zeros([agentN,agentN])\n","    for i in Idx:\n","        G[i[0],i[1]] = 1 \n","    G=G+G.T\n","    d=G.sum(axis=1)\n","    # Construct the mixing matrix\n","    W=np.zeros([agentN,agentN])\n","    for i in Idx:\n","        W[i[0], i[1]] = 0.5/max(d[i[0]], d[i[1]])\n","    W=W+W.T\n","    diag=np.eye(agentN,agentN)\n","    for i in range(agentN):\n","        diag[i,i] = 1 - np.sum(W[i])\n","    W=W+diag\n","    return W\n","\n","\n","epsilon=0.45\n","p=(1+epsilon)*ma.log(agentN)/agentN\n","C=0\n","\n","def erdos(N, p):\n","    A=np.random.rand(N,N)<p\n","    A=np.triu(A, 1)\n","    A=A+A.T\n","    Lap=np.diag(A*np.ones((N, 1)))-A\n","    V_L, Lambda_L=np.linalg.eig(Lap)\n","    if Lambda_L[2,2]<=1e-8:\n","        C=0\n","    else:\n","        C=1\n","    return A, C\n","\n","def generate_conn_w(agentN, N, p):\n","    C=0\n","    while C==0:\n","        A1,C=erdos(N, p)\n","    d=A1.sum(axis=1)\n","    # Construct the mixing matrix\n","    W=np.zeros([agentN,agentN])\n","    A2=A1-np.diag(A1*np.ones((N,1)))\n","    Idx_n=np.argwhere(A1)\n","    for i in Idx_n:\n","        W[i[0], i[1]] = 0.5/max(d[i[0]], d[i[1]])\n","    diag=np.eye(agentN)\n","    for i in range(agentN):\n","        diag[i,i] = 1 - np.sum(W[i])\n","    W=W+diag\n","    return W\n","\n","Idx=[]\n","for i in range(agentN):\n","    for j in range(i+1, agentN):\n","        elem=np.array([[i, j]])\n","        Idx=Idx+elem.tolist()\n","\n","print(Idx)\n","l = [1, 2, 3, 4, 5]\n","fracx=4\n","\n","\n","def compute_gamma_e(w, i, frac):\n","    if frac==0:\n","        return 1/w[i]\n","    else:\n","        w_list=list(w)\n","        w_list.pop(i)\n","        w_inverse=np.asarray(list(combinations(w_list, frac)))\n","        w_inverse=[w_inverse[k]+w[i]/len(w_list) for k in range(w_inverse.shape[0])]\n","    #    print(w_inverse)\n","    #    w_inverse2=np.concatenate((w_inverse,w[i]*np.ones(w_inverse.shape[0]).T))\n","        w_inverse_sum=1/((np.sum(w_inverse, axis=1)))\n","        return np.sum(w_inverse_sum)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztTqos7MBbYf","colab_type":"code","colab":{}},"source":["# Consensus - Directed DGD\n","\n","agentN=10\n","x_star_init=np.array([10*np.random.random(size=paraN)-5 for j in range(agentN)])\n","x_star_init=np.array([[x_star_init[i][j] if abs(x_star_init[i][j])<5 else 5 for j in range(paraN)] for i in range(agentN)])\n","x_star_consensus_ave=np.sum(x_star_init, axis=0)/agentN\n","x_star_consensus=np.array([x_star_consensus_ave for j in range(agentN)])\n","p=0.75\n","\n","z_init=np.vstack((x_star_init, np.zeros((agentN, paraN))))\n","z_star_consensus_ave=np.sum(x_star_init, axis=0)/(agentN)\n","z_star_consensus=[z_star_consensus_ave if k< agentN else np.zeros((1, paraN))[0] for k in range(2*agentN)]\n","#print(z_star_consensus)\n","\n","\n","N=5000\n","epsilon=0.01\n","\n","x_a1_cons=x_star_init\n","y_a1_cons=np.ones((agentN, 1))\n","x_a1_hat=np.zeros((agentN, paraN))\n","x_a2_cons=x_star_init\n","#mu, sigma = 0, 0.0001\n","\n","\n","def directed_dp_con(N, sel_num, agentN, paraN, epsilon, sigma, mode):\n","    z_a1_cons=z_init\n","    loss_z_cons=[]\n","    for i in range(int(N)):\n","        z_a1_cons_col=np.zeros((agentN*2, paraN))\n","        for j in range(paraN):\n","            # Construct Matrix M\n","            # Mechanism 1\n","            A=np.random.random((agentN,agentN))\n","            B=np.random.random((agentN, agentN))\n","            A_new=np.array([A[k] for k in range(agentN)])\n","            B_new=np.array([B[k] for k in range(agentN)])\n","            # Mechanism 2\n","#            p=0.95\n","#            Wk=generate_conn_w(agentN, agentN, p)\n","#            A_new=np.array([Wk[k] for k in range(agentN)])\n","#            B_new=np.array([Wk[k] for k in range(agentN)])\n","            \n","            for m in range(agentN):\n","                w_list=list(range(agentN))\n","                w_list.pop(m)\n","                if sel_num=='full':\n","                    w_num=0\n","                elif sel_num=='none':\n","                    w_num=agentN-1\n","                else:\n","                    w_num=sel_num\n","                w_id=random.sample(w_list,w_num)\n","\n","                A_new[m][w_id]=0\n","                B_new[m][w_id]=0\n","\n","            A=np.array([A_new[k]/sum(A_new[k]) for k in range(agentN)]) # Make it row-stochastic            \n","            B=np.array([B_new.T[k]/sum(B_new.T[k]) for k in range(agentN)]) #Make it column-stochastic\n","            B=B.T\n","            s_idx=np.random.binomial(1, 0.9)\n","            if s_idx==1:\n","                A=np.eye(agentN)\n","                B=np.eye(agentN)\n","            M_up=np.hstack((A, epsilon*np.eye(agentN)))\n","            M_down=np.hstack((np.eye(agentN)-A, B-epsilon*np.eye(agentN)))\n","            M_new=np.vstack((M_up, M_down))\n","        #    M_1=np.array([M_new[k] for k in range(2*agentN)])\n","            s = np.random.normal(0, sigma, (agentN, 1))\n","#            z_a1_cons_col_new=np.array([z_a1_cons[k, j]+s[k, 0] if k<agentN else z_a1_cons[k, j]-s[k-agentN, 0] for k in range(agentN*2)])\n","#            z_a1_cons_col[:, j]=M_new.dot(z_a1_cons_col_new.T)\n","            z_a1_cons_col[:, j]=M_new.dot(z_a1_cons[:, j])\n","            if i==0 and mode=='dp':\n","                z_a1_cons_col_noise=np.array([s[k, 0] if k<agentN else -s[k-agentN, 0] for k in range(agentN*2) ])\n","                z_a1_cons_col[:, j] += z_a1_cons_col_noise\n","            \n","\n","        #    z_a1_cons=M_new.dot(z_a1_cons)\n","        z_a1_cons=np.array([z_a1_cons_col[k] for k in range(2*agentN)])\n","        z_a1_ave=np.sum(z_a1_cons, axis=0)/(agentN)\n","        z_a1_consensus=np.array([z_a1_ave for j in range(agentN)])\n","        z_a1_consensus_xy=np.vstack((z_a1_consensus, np.zeros((agentN, paraN))))\n","        #    print(z_a1_cons.shape, z_a1_consensus.shape)\n","#        loss_z_cons.append(LA.norm(z_a1_cons-z_a1_consensus_xy))\n","        loss_z_cons.append(LA.norm(z_a1_cons-z_star_consensus))\n","        print(i)\n","    return loss_z_cons\n","    \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2aTTTOGrBhY-","colab_type":"code","colab":{}},"source":["# Run experiments with different parameters and plot\n","\n","N=5000\n","#loss_z_cons_f=directed_dp_con(N, \"full\", agentN, paraN, 0.01, 1, 'f')\n","#loss_z_cons_r=directed_dp_con(N, \"full\", agentN, paraN, 0.01, 3, 'dp')\n","#loss_z_cons_r1=directed_dp_con(N, agentN-5, agentN, paraN, 0.01, 1, 'f')\n","#loss_z_cons_r2=directed_dp_con(N, agentN-5, agentN, paraN, 0.01, 3, 'dp')\n","\n","\n","start=1\n","y_axis = np.random.normal(loc=0.5, scale=0.4, size=1000)\n","y_axis = y_axis[(y_axis > 0) & (y_axis < 1)]\n","#plt.ylim((1e-10, 1))\n","y_axis.sort()    \n","plt.clf\n","#plt.ylim((1e-12, 100))\n","#plt.xlim((1, 2600))\n","#plt.xscale('log')\n","plt.yscale('log')\n","plt.grid()\n","\n","plt.plot(loss_z_cons_f, color='r', linestyle=\"-\", label =\"Full Communication\")\n","plt.plot(loss_z_cons_r, color='g', linestyle=\"-.\", label =\"Full Communication Privacy\")\n","plt.plot(loss_z_cons_r1, color='y', linestyle=\"-.\", label =\"0.5 Communication \")\n","plt.plot(loss_z_cons_r2, color='b', linestyle=\"-\", label =\"0.5 Communication Privacy\")\n","\n","plt.xlabel('iteration t')\n","plt.ylabel('Residual')\n","# plt.legend()\n","plt.legend(bbox_to_anchor=(1.01, 0.70),loc='upper right')\n","plt.savefig('dp_consensus.eps',dpi=600,format='eps')"],"execution_count":0,"outputs":[]}]}